---
title: 'ECON312 Problem Set 1: question 3'
author: Futing Chen, Hongfan Chen, Will Parker
date: "`r format(Sys.time(), '%m/%d/%Y')`"
output:
  pdf_document:
    toc: yes
    toc_depth: '2'
  html_notebook:
    toc: yes
    toc_depth: 2
    toc_float: yes
---



```{r setup, include=FALSE}
knitr::opts_chunk$set(cache = TRUE, echo = TRUE, warning = FALSE, message = FALSE)
```

```{r, cache = FALSE}
library(tidyverse)
library(knitr)
```


\pagebreak
# Monte Carlo Simulations

Consider the model:

$$Y_i = X_i'\beta + U_i$$
$$U_i|X_i \overset{i.i.d}{\sim} N(0, \sigma^2) $$

## Part a) 
Define $\beta = (2,3)^T$, $\sigma^2 = 4$; generate $N = 10,000$ values for $X \in \mathbb{R}^2$. Using your value for $\sigma^2$ draw $U$'s

```{r}
set.seed(123456)
# doesn't appear that the distribution of X was specified
# I just used standard normal for x_1
N <- 10000


X_0 <- rep(1, N)
X_1 <-rnorm(n = N)

sigma <- 2
U <- rnorm(n = N, sd = sigma)

data <- tibble(X_0 = X_0,
               X_1 = X_1,
               U = U) 

kable(head(data))
```

Finally, compute the $Y$'s
```{r}
beta <- c(2,3)

data <- data %>%
  mutate(Y = X_0*beta[[1]] + X_1*beta[[2]] + U)

knitr::kable(head(data))
```

### Estimate $\hat{\beta}$ and its standard errors from your data using standard OLS formulas.

We did the actual matrix calculations

#### $\hat{\beta} = (XX')^{-1}(XY)$
```{r}

X <- as.matrix(tibble(int =1,
            X_1 = data$X_1))

Y <- data$Y

beta_n <- solve(t(X)%*%X)%*%t(X)%*%Y

kable(beta_n, col.names = "Beta")
```

#### Standard errors 

Under homoskedasticity which is given in the model,


$$V = (XX')^{-1}\hat{\sigma}^2$$

$$se(\hat{\beta_k}) = \sqrt{\frac{1}{n}diag(\hat{V})_k}$$
```{r}
u <- Y - X%*%beta_n

u_sq <- as.vector(u *u)

sigma_sq_hat <- sum(u_sq)/N

V <- solve(t(X)%*%X)*sigma_sq_hat

se <- sqrt(diag(V))

kable(se, col.names = "standard error")
```

### Verifying with statistical software
```{r}
ols <- lm(Y ~ X_1, data)

summary(ols)
```


## Part b)

Write a function to generate the $\hat{\beta}^{(s)}$s
```{r}

beta_sampler <- function(N = 10000, sigma = 2, beta =  c(2,3)){
  # we used standard normal for x_1
  X_0 <- rep(1, N)
  X_1 <-rnorm(n = N)
  
  
  U <- rnorm(n = N, sd = sigma)
  
  data <- tibble(X_0 = X_0,
                 X_1 = X_1,
                 U = U) 
  data <- data %>%
    mutate(Y = X_0*beta[[1]] + X_1*beta[[2]] + U)
  
  X <- as.matrix(tibble(int =1,
              X_1 = data$X_1))
  
  Y <- data$Y
  
  beta_n <- solve(t(X)%*%X)%*%t(X)%*%Y
  
  return(beta_n)
}
```

```{r, cache=TRUE}
S <- 10000

beta_0_list <- vector(mode = "numeric" , length = S)
beta_1_list <- vector(mode = "numeric" , length = S)

set.seed(123456)

for (k in seq(1:S)) {
  current_sample <- beta_sampler()  
  
  beta_0_list[k] <- current_sample[[1]]
  beta_1_list[k] <- current_sample[[2]]
  
}
```

```{r}
tibble(beta_0 = beta_0_list) %>%
  ggplot(aes(x = beta_0)) + 
  geom_histogram()
```

### Standard error of $\hat(\beta_k)$

Proof of consistency:


First we need to justify that 

$$\sqrt{\frac{1}{S}\sum_{s=1}^S (\hat\beta_k^{(s)})^2 - (\frac{1}{S}\sum_{s=1}^S \hat\beta_k^{(s)})^2} \overset{p}{\to} se(\hat{\beta_k}| X_1, X_2,..X_n)$$

First note by WILLIN that because $\beta_k$ is a random variable

$$\frac{1}{S}\sum_{s=1}^S (\hat\beta_k^{(s)}) \overset{p}{\to} E[\beta_k]$$

So then by the continuous mapping theorem

$$\frac{1}{S}\sum_{s=1}^S (\hat\beta_k^{(s)})^2 \overset{p}{\to} E[\beta_k^2]$$

And again by the continuous mapping theorem

$$\sqrt{\frac{1}{S}\sum_{s=1}^S (\hat\beta_k^{(s)})^2 - (\frac{1}{S}\sum_{s=1}^S \hat\beta_k^{(s)})^2} \overset{p}{\to} \sqrt{E[\beta_k^2] - E[\beta_k]^2}  = se(\hat{\beta_k}| X_1, X_2,..X_n)$$



### computing $\sqrt{\hat{Var}[\hat{\beta}^{(s)}]}$

#### $se(\hat{\beta_{0}})$
```{r}
sqrt(mean(beta_0_list^2) - mean(beta_0_list)^2)
```

#### $se(\hat{\beta_{1}})$
```{r}
sqrt(mean(beta_1_list^2) - mean(beta_1_list)^2)
```
Very close to standard error produced by OLS procedure 

# Nonparametric Bootstrap

## Part a)
```{r}
set.seed(123456)
rct_sample <- function(N = 10000){
  U_1 <- rnorm(n = N)
  U_2 <- rnorm(n = N)
  
  assignment <- runif(n = N)
  
  sample <- tibble(U_1,
                   U_2,
                   assignment) %>%
    mutate(Y_1 = 5 + U_1,
           Y_0 = 2 + U_2,
           D = ifelse(assignment>0.5, 1, 0),
           beta = Y_1 - Y_0,
           Y = Y_0 + D*beta)

  
  return(sample)
  
}

single_sample <- rct_sample()

summary(lm(Y ~ D, data = single_sample))
```

Proof that OLS estimates are consistent

Let $m$ be the number of treated individuals and $k$ be the number of untreated individuals. Then the OLS estimate of the treatment effect is

$$\hat{\beta}_{OLS} =  (\frac{1}{m}*\sum_1^m Y_{D=1}) - (\frac{1}{k}*\sum_1^k Y_{D=0} )$$

Because we have assigned D randomly and $D \!\perp\!\!\!\perp  (Y_1,Y_0)$, we can plug in our equations for $Y_1$ and $Y_0$

$$\hat{\beta}_{OLS} =  (\frac{1}{m}*\sum_1^m 5 + U_1) - (\frac{1}{k}*\sum_1^k 2+ U_2)$$

Since we are definining $P(D=1) = 0.5$, without loss of generality we can set $m = k = n/2$.  so we can simplify to

$$\hat{\beta}_{OLS} =  3 + (\frac{1}{m}*\sum_1^m U_1) - (\frac{1}{k}*\sum_1^kU_2)$$
because $U_1$ and $U_2$ are standard normal variables, by the WILLIN 
$$\frac{1}{m}*\sum_1^m U_1 \overset{p}{\to} 0$$
$$\frac{1}{k}*\sum_1^k U_2 \overset{p}{\to} 0$$

So by the continuos mapping theorem

$$\hat{\beta}_{OLS} \overset{p}{\to}  3 = E[Y_1 - Y_0]$$
so OLS gives a consistent estimate of the average treatment effect

## Part b)
```{r, cache = TRUE}
N <- 10000
S <- 10000
  
beta_list <- vector(mode = "numeric", length = S)
  
for (k in seq(1:S)) {
    bootstrap <- single_sample %>%
      select(Y, D) %>%
      sample_n(size = N, replace = TRUE)
      
    model <- lm(Y ~ D, data = bootstrap)
    

    beta_list[[k]] <- model$coefficients[[2]]
}
```


### Standard error of $\hat{\beta}$
```{r}
sqrt(mean(beta_list^2) - mean(beta_list)^2)
```


### Histogram
```{r}
tibble(beta = beta_list) %>%
  ggplot(aes(x = beta)) +
  geom_histogram()
```



### bad boostrap

If $Y$ and $D$ were drawn independently from the sample, then treatment assignment would no longer be related to the observed outcome. We would be running a regression on $(Y_1, D_0)$, $(Y_1, D_1)$, $(Y_0, D_0)$, and $(Y_0, D_1)$ with equal probability (since $P(D =1) = 0.5$ in our example). This would lead to a estimate of a treatment effect of 0.

```{r, cache = TRUE}

N <- 10000
S <- 10000
  
bad_boot_strap_list <- vector(mode = "numeric", length = S)
  
for (k in seq(1:S)) {
    Y <- sample(single_sample$Y, size = N, replace = TRUE)
    D <- sample(single_sample$Y, size = N, replace = TRUE)
    
    
    model <- lm(Y ~ D)
    

    bad_boot_strap_list[[k]] <- model$coefficients[[2]]
}

tibble(beta = bad_boot_strap_list) %>%
  ggplot(aes(x = beta)) +
  geom_histogram()
```



