---
title: 'ECON312 Problem Set 1B: question 5'
author: Futing Chen, Hongfan Chen, Will Parker
date: "`r format(Sys.time(), '%m/%d/%Y')`"
output:
  pdf_document:
    toc: yes
    toc_depth: '2'
  html_notebook:
    toc: yes
    toc_depth: 2
    toc_float: yes
---


```{r setup, include=FALSE}
knitr::opts_chunk$set(cache = FALSE, echo = TRUE, warning = FALSE, message = FALSE)
```

```{r, cache = FALSE}
library(tidyverse)
library(knitr)

library(readxl)
```


Load in data
```{r}
sheets <- excel_sheets("PS1_Q5_Data.xlsx")


dataset_list <- list()

for (s in seq(1:length(sheets))) {
  dataset_list[[s]] <- readxl::read_excel("PS1_Q5_Data.xlsx", sheet = s) %>%
    mutate(dataset_num = s) %>%
    select(Y, X1, X2, dataset_num)
}
```



# A: Pre-test estimator
```{r}
sample_params <- function(df){
  
  
  dataset_num <- filter(df, row_number() ==1)$dataset_num
  
  n <- df %>% nrow()
  
  mu_1 <- mean(df$X1)
  mu_2 <- mean(df$X2)
  
  sigma2_1 <- var(df$X1)
  sigma2_2 <- var(df$X2)
  rho <- cov(df$X1, df$X2)/sqrt(sigma2_1*sigma2_2)
  
  m_1 <- lm(data = df, formula = formula(Y ~ X1 + X2))
  
  sigma2_epsilon <- mean(m_1$residuals^2)
  
  beta_1_hat <- m_1$coefficients[["X1"]]
  
  
  if (is.na(m_1$coefficients[["X2"]]) == FALSE){
    t_beta_2 <- summary(m_1)$coefficients[["X2", "t value"]]
  } else { t_beta_2 <- 0}
  
  
  m_2 <- lm(data = df, formula = formula(Y ~ X1))
  
  beta_1_tilda <- m_2$coefficients[["X1"]]

  
  if (abs(t_beta_2) > 1.964) {
    beta_1_star <- beta_1_hat
    
    Q_xx <- matrix(nrow =2, c(sigma2_1, sqrt(sigma2_1*sigma2_2)*rho, sqrt(sigma2_1*sigma2_2)*rho, sigma2_2))
      
    std_err_beta_1_star <- sqrt(sigma2_epsilon*solve(Q_xx)[[1,1]]/n)
    
    analytic_bias <- 0
  }  else {
    beta_1_star <- beta_1_tilda
    
    analytic_bias <- (1)*(rho*sqrt(sigma2_2/sigma2_1))
    #std_err_beta_1_star <-
  }
  

  
  output <- tibble(dataset_num, 
                   mu_1,
                   mu_2,
                   sigma2_1,
                   sigma2_2,
                   rho,
                   sigma2_epsilon,
                   t_beta_2,
                   beta_1_hat,
                   beta_1_tilda,
                   beta_1_star,
                   analytic_bias) %>% 
    mutate(empiric_bias = beta_1_star -1)
  
  return(output)
}


```


## Test that function is working
```{r}
summary(lm("Y~ X1 + X2", dataset_list[[2]]))
```


```{r}
summary(lm("Y~ X1", dataset_list[[2]]))
```

```{r}
sample_params(dataset_list[[2]])
```


```{r}
results <- map_dfr(dataset_list, sample_params)

results %>%
  kable(col.names = c("Dataset", "$\\mu_1$", "$\\mu_2$", "$\\sigma^2_1$", "$\\sigma_2^2$", "$\\rho$", "$\\sigma_{e}^2$", "$t_{\\beta_2}$", "$\\hat{\\beta}_1$", "$\\tilde{\\beta}_1$", "$\\beta_1*$", "analytic bias", "empiric bias"), digits = 2)
```



## Distribution of $\beta_1^*$ across the 72 samples
```{r}
results %>%
  ggplot(aes(x  = beta_1_star)) +
  geom_histogram()
```



## Sampling distribution for the pre-test estimator

If $|t|_{\hat{\beta_2}} > 1.96$, then $\beta_1^*$ has the typical OLS asymptomic variance, i.e. for $\hat{\beta} = (\hat{\beta}_1, \hat{\beta}_2)$

$$\sqrt{n}(\hat{\beta}_n - \beta) \overset{d}{\to} N(0, \sigma^2_{\epsilon}*E[X'X]^{-1}) $$

In terms of the model parameters, we can write

$$E[X'X] = (\begin{matrix}
 \sigma_1^2 & \rho \sigma_1 \sigma_2 \\ \rho \sigma_1 \sigma_2 & \sigma_2^2
\end{matrix})$$

And calculate the limiting distribution in this case

If $|t|_{\hat{\beta_2}} \leq 1.96$, then the variance is more complex

...




# B

## Analytic Bias of $\beta_1^*$

If $|t|_{\hat{\beta_2}} > 1.96$, then $\beta_1^* = \hat{\beta}_1$ which is unbiased, i.e.

$$E[\hat{\beta}_1] = \beta_1$$

If $|t|_{\hat{\beta_2}} \leq 1.96$, then $\beta_1^* = \tilde{\beta}_1$ which has the standard missing variable bias

$$E[\tilde{\beta}_1] = \beta_1 + \beta_2\frac{Cov(X_1,X_2)}{Var(X_1)}$$
based on the data geneterating process we know

$$ Cov(X_1,X_2)= \rho\sigma_1 \sigma_2$$
$$Var(X_1) = \sigma_1^2$$

So by solving we have the bias

$$E[\tilde{\beta}_1] = \beta_1 + \beta_2\frac{\rho \sigma_2}{\sigma_1}$$

So then the $E[\beta_1^*]$

$$E[\beta_1^*]= P(|t|_{\hat{\beta_2}} > 1.96)*\beta_1 + P(|t|_{\hat{\beta_2}} \leq 1.96)*(\beta_1 + \beta_2\frac{\rho \sigma_2}{\sigma_1})$$

...plug in for P(t>1.96)...


## Parameter distribution in the datasets
```{r}
results %>%
  ggplot(aes(x = rho)) +
  geom_histogram()
```

```{r}
results %>%
  ggplot(aes(x = sigma2_epsilon)) +
  geom_histogram()
```


```{r}
results %>%
  ggplot(aes(x = sigma2_1)) +
  geom_histogram()
```

```{r}
results %>%
  ggplot(aes(x = sigma2_2)) +
  geom_histogram()
```

## Relationship of parameters to observed bias



```{r}
results %>%
  ggplot(aes(x = sigma2_epsilon, y = abs(empiric_bias))) + 
  geom_point() + labs(x = "sigma2_epsilon", y = "|bias|") 
```


## fixing $\rho = 1$ and estimating bias as a function of $\sigma^2_{epsilon}$
```{r}
results %>%
  filter(rho ==1) %>% 
  ggplot(aes(x = sigma2_epsilon, y = abs(empiric_bias))) + 
  geom_point() + labs(x = "sigma2_epsilon", y = "|bias|") 
```


```{r}

results %>%
  ggplot(aes(x = rho, y = sigma2_2/sigma2_1,  color = abs(empiric_bias))) + 
  geom_point() + labs(x = "p", y = "var(X2)/var(X1)", color = "|observed bias in beta1|") +
  scale_color_distiller(palette = )
```


# C

A bayesian would assume a prior distribution for $\theta = (\beta_0, \beta_1, \beta_2)$, e.g.

$$P(\theta) = N(0, \Sigma)$$

Then compute the posterior distribution of $P(\theta|(\textbf{X}, \textbf{Y}))$ via bayes formula

$$P(\theta|(\textbf{X}, \textbf{Y})) = \frac{1}{Z}f(\theta|(\textbf{X}, \textbf{Y}))*P(\theta)$$

Where 

$$ Z = \int_X \int_Y f(\theta|(\textbf{X}, \textbf{Y})) dY dX $$

Then the Bayesian could do testing of any specific hypothesis on $\beta_1$ or $\beta_2$ with corresponding posterior marginal probability distribution, e.g. for the hypothesis that $\beta_1$ is greater than 1

$$P(\beta_1 > 1) = \int_1^{\infty} P(\beta_1|(\textbf{X}, \textbf{Y})) dX_1 $$
